Welcome to the First Principles video and blog series where we talk about various architectural aspects behind OCI services. At OCI, we take a lot of pride in delivering the maximum performance possible for the least cost possible to customers.

Remote DMA, or RDMA, as we call it, has been a core technology behind many OCI services pretty much from the beginning. And we actually recently released a video blog talking about various architectural aspects, historical aspects right from the beginning about why we decided to invest in RDMA and the various products that use RDMA and various trade-offs associated with that.

RDMA is essentially, in a nutshell, is a technology that allows for data transfer or network communication that bypasses CPU, goes from one machine to another without any CPU interference. And this allows things like GPUs to communicate at extremely low latency, high bandwidth with low overhead from a CPU perspective.

And this has actually been a foundational building block for our database services, such as ExaCS and Autonomous Database services and as well as for HPC workloads, as well as GPU workloads. And a few years back, we decided to make a strategic bet on Rocky. Rocky stands for RDMA or converged ethernet. So right now, we essentially have an ethernet fabric that essentially enables Rocky. And our HPC workloads, GPU workloads, as well as database workloads all run on-- leverage that and run on top of it.

In the last Cloud World, we announced a strategic partnership with NVIDIA around GPU workloads. And in that context, we've been talking to customers as well as NVIDIA. And it's become very clear that there is a clear demand for very high scale GPU workloads that spans maybe thousands or maybe even tens of thousands of GPUs that can essentially run within a single RDMA network, if you will.

It's essentially like a Supercluster that essentially supports RDMA. And in order to cater that workload, we went back and decided to make some engineering and architectural design enhancements to roll out what we call as RDMA Supercluster that is designed to support very large number of GPUs. To talk about some of the architectural aspects of our Supercluster, we have with as again Jack, who is the lead architect for RDMA networking. Welcome, Jack.

Thank you, Pradeep. I'm here to represent the team.

As always. All right, so tell us a little bit more about the Superclusters.

Yes, so let's take a look at schematic here. This is probably a high level drawing. So what we're showing here is we're showing two GPU nodes at the bottom where each node is comprised of eight NVIDIA A100 class of GPUs that are interconnected using NVIDIA's NVLinks. This is NVLinks.

And the GPU nodes themselves connect to what we call the network fabric. And the way to think about this is network fabric provides a nonblocking interconnect between all the GPUs, meaning the fabric can provide all the benefit that all the GPU nodes need. And in this case, each GPU node connects to the fabric at 1.6 terabits per second or 1,600 gigabits per second. That's like the high level view. Now let's take a look at what the fabric looks like.

So this is like for every GPU that's 200 gigabits per second?

Correct. Each of the GPU gets 200 gigabits of proportional bandwidth because there's a total of 1.6 G.

Right. And it's fully operational. And it's nonblocking in the sense that any GPU in the fabric can talk to any other GPU fabric at a time.

At the same time, yes.

Good observations.

So looking at-- this is the next-level picture of the fabric. So here we look at what the fabric looks like. Now, we use the word Supercluster. By Supercluster what we mean is, it's much larger than an existing cluster network. In particular, we have a concept of blocks. So there's block number one here and block number n here.

Each of the blocks have two tiers of what we call the Clo fabric. Clo is a particular kind of network design. This is a three-tier Clo network. So this network, we plan to scale to tens of thousands of GPUs. And we can envision designs and scale over 100,000 GPUs as well.

Got it. So this is essentially a three-tier Clo network. And so thinking about this, let's say a GPU all the way over here in the block number one wants to talk to the GPU at the very right side of the n, the GPU block, if you will. Well, I can imagine there's going to be additional network latency because of this three-tier Clo design correct in terms of the number of hops. So how do you actually manage that?

Very good point. So you observed correctly. Because this Supercluster is so large, we're talking tens of thousands GPUs. Potentially, we can talk over 100,000 GPUs. That's a lot of GPUs. And just physical constraints, as in power and space, would mean, we have a long what's called a cable distance.

So the worst-case latency in this case, we're talking around 20 microseconds round trip. And as we previously noted in the other blog, the latency within a block is about 6.5 microseconds round trip. So latency has gone up, but still we're talking not very high latency.

Now, in order to account for that higher latency, what we're doing is we have-- because this is QOS-enabled network, as we talked about in the other blog, we have made sure that the switches and the silicon and the chipsets we chose have the right amount of buffering. And we have made sure to allocate the right amount of buffers to the queue that the NLGPU workloads would use. So we have taken care of the higher worst-case latency here.

Got it. So this is not just like a regular three-tier Clo which has been around for a while. It's essentially something that's designed so that we can do lossless networking, if you will, using higher buffers that are specifically tuned across all these devices in order to make this three-tier Clo Supercluster work in conjunction with RDMA.

Very true. You used the right word, the word that I missed. The operating word is lossless. So this is a lossless RDMA network. And lossless networking means the switches don't drop packets. And they have intelligent congestion notification system built into it to avoid packet loss. And all of that still applies. But we just made sure we account for the worst-case higher latency, so the packets are never dropped still.

Cool. So that's great for-- 20 microseconds is still pretty great compared to any other virtualized networking, if you will. Typically, that's like an order of magnitude better than a cross AD virtual network that we might find. But still there are workloads that are extremely sensitive to latency. And they still may want the 6, 7 microsecond latency, How do we deal with that in the Supercluster.

Very good point. Firstly, as you said, the latency, the worst-case latency we talked about is still 10 to 20 times lower than what you would typically experience in a cloud network. But in order to support workloads that need lower latency or that don't really need the scale we're talking about here, imagine you have a small GPU cluster. You don't need tens of thousands of GPUs. You need maybe 1,000 GPUs, or an HPC workload, or an Oracle database workload that maybe doesn't need that larger scale or really cares about low latency.

So what we do for those workloads is we have a control plane that figures out where to deploy a customer's workload. And it takes into account this latency. So if you have a small-enough workload that fits in a cluster, it would only be deployed in what we call a single block. So you would then experience--

Like a HPC or a database cluster workloads essentially go into a single block.

Exactly, or even a smaller workload. It'll go in a single block. And it'll experience the lower latency. So we struck the balance between scale and latency using what we call placement.

Yeah, makes sense. Cool. How about GPU workloads, though? Let's say they actually span multiple of these blocks. Are they essentially going to have to deal with the 20 microsecond latency all the time?

Very good point. And the answer is not all the time. And let's take a look at how we do that. In fact, we believe it's an innovative thing we've done. So as we talked about in the other blog, we have this thing called network locality hints. So we have a service that provides network locality information to the customers and help them build, for example, their GPU topology, their ML topology to take advantage of that.

So let's take a look at that here. So here we're showing a workload that has eight hosts behind four different top of the rack switches in two different blocks. So this is one block. That's another block. Now, if the workload didn't know anything about locality, then any GPU could be talking to any other GPU at the same time. And you could have relatively higher latency. Still lower numbers but higher than what it needs to be.

So what we've done is we make network locality transparent to the customer. We make that information available to the customer. And what in this case the customer has done is they've interconnected their GPUs in a way where two GPUs on the top of the rack switch, half of their traffic stays local to the top of the rack switch.

For GPUs within the block, something like 85% of the traffic stays local. If you look at all these links, they're local to the block. Just occasionally, some of these links go across blocks. And what this means is most of the time, most of the traffic has really low latency.

Occasionally, some of the traffic has that 20-ish microsecond latency. But the end result is on average, your latency is much lower. In fact, half of the traffic has even less than 6.5 microseconds of latency because they're local to the top of the rack switch. So this is what we call network locality or placement hints.

Got it. And if there are a lot of the traffic is actually constrained within a tour or a block, that also helps reduce flow collisions at many levels of these, the three--

Very good observation. So we got a very nice side benefit, which is customers get really high throughput because this thing called flow collision, which is a common phenomena in all networks where two flows could collide on a single link. The probability of that is much reduced. So you not only get lower latency, you get higher throughput.

Got it. Cool. Well, to summarize, essentially the Supercluster is a three-tier cloud network. But it's a whole bunch of optimizations?

Correct.

The three big things that I think I heard were, one, we have buffers that are tuned for the approximate latency diameter in the network itself so that we can actually preserve the lossless aspect of the Supercluster.

Correct.

And second, we have placement mechanism in our control plane where when you launch GPU nodes or maybe database nodes or whatever, they actually tend to as much as possible be placed within the same block or maybe even within the same tour to reduce the latency as well as to decrease the probability of flow collisions across the network--

That's higher throughput.

--and which results in higher throughput. And then the third, we have even for workloads that actually span multiple blocks or tours. We actually have placement hints that can be passed down to the orchestration mechanism like the GPU workloads where the algorithms can essentially use the locality within the orchestration itself to limit the traffic within tours or maybe blocks as much as possible, reducing the latency as well as reducing the probability of flow collision.

Correct. You said it better than I could have. Thank you.

Cool. Thank you, Jack, appreciate--

My pleasure.

--your time.